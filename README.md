# babyGPT
A tiny, handcrafted language model built from scratch using a whitespace tokenizer and simple Transformer architecture. Just enough intelligence to tell bedtime stories and mispredict verbs. Ideal for learning, not for launching rockets.

# ðŸ§  MiniLLM â€” A Tiny Language Model Built From Scratch

Welcome! This project is a personal exploration into building a simple, from-scratch Transformer-based Language Model (LM) using only custom, controlled input data (e.g., childrenâ€™s stories). It's a lightweight NLP system with a custom tokenizer and training pipeline designed for education and experimentation.

---

## ðŸ§± Project Architecture


---

## ðŸ”¤ Tokenization Strategy

- **Tokenizer Type:** Whitespace-based (each word = one token)
- **Vocabulary:** Built from the raw dataset
- **Special Tokens:** `<pad>`, `<unk>`, `<bos>`, `<eos>`

---

## âœ… Goals

- Understand how LLMs work from scratch
- Build a minimal but functioning language model
- Document the full development process
- Use a simplified architecture for educational clarity

---

## ðŸ“Œ Notes

- This is not a production model.
- All components are handcrafted for transparency and control.
- Ideal for learning, testing, and experimenting with NLP.

---

## ðŸš§ Roadmap

- [x] Setup project structure
- [ ] Implement whitespace tokenizer
- [ ] Build vocabulary from dataset
- [ ] Encode stories into token sequences
- [ ] Train a 1-layer Transformer
- [ ] Generate simple sentences

---

## ðŸ§  Author

Eduardo â€” engineer, builder, and curious mind exploring the inner mechanics of intelligence.





